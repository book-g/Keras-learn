<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Layer Weight Initializers</title>
    <link rel="stylesheet" href="../../../styles.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.2/katex.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.2/katex.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.2/contrib/auto-render.min.js"></script>
  
</head>
<body>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    { left: "$$", right: "$$", display: true },
                    { left: "$", right: "$", display: false }
                ]
            });

            document.querySelectorAll('.show-more-btn').forEach(button => {
                button.addEventListener('click', function(event) {
                    event.stopPropagation(); // Prevents the click event from propagating to the parent <a> element
                    event.preventDefault();  // Prevents the default action of the button, if it has a default action
                    
                    const card = this.closest('.card');
                    const content = card.querySelector('.card-content');
                    const isHidden = content.classList.contains('hidden-content');

                    if (isHidden) {
                        content.classList.remove('hidden-content');
                        this.textContent = "hide";
                    } else {
                        content.classList.add('hidden-content');
                        this.textContent = "show";
                    }
                });
            });
        });
    </script>
    <div class="layout">
        <main class="content-area">
            <h1>Layer Weight Initializers</h1>
            <div class="card-grid">
                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/01. RandomNormal class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/01. RandomNormal class/random_normal_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>RandomNormal</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>RandomNormal Initializer in Keras</h1>
                        
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>RandomNormal</code> initializer generates weights according to the normal distribution defined by the following parameters:</p>
                                    <ul>
                                        <li><b>Mean (μ)</b>: The mean of the normal distribution.</li>
                                        <li><b>Standard Deviation (σ)</b>: The standard deviation of the normal distribution.</li>
                                    </ul>
                                    <p>The formula for the weights <code>W</code> initialized using <code>RandomNormal</code> is:</p>
                                    <div class="equation">
                                        \[
                                        W \sim \mathcal{N} (\text{mean}, \text{stddev}^2)
                                        \]
                                    </div>
                                    <p>where \( \mathcal{N} \)  denotes a normal distribution with:</p>
                                    <ul>
                                        <li><b>Mean (μ)</b>: The average value of the distribution.</li>
                                        <li><b>Variance (σ²)</b>: The square of the standard deviation, representing the spread of the distribution.</li>
                                    </ul>
                                </div>
                        
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>RandomNormal</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>Why Use Normal Distribution?</b> Initializing weights with a normal distribution helps in breaking the symmetry and can improve convergence speed during training.</li>
                                                <li><b>Impact on Training:</b> The choice of mean and standard deviation can affect the performance and convergence of the model. For example, weights that are too large or too small can lead to problems such as vanishing or exploding gradients.</li>
                                            </ul>
                                        </li>
                                        <li><b>Default Values</b>:
                                            <ul>
                                                <li><b>Mean:</b> Default is 0.0.</li>
                                                <li><b>Standard Deviation:</b> Default is 0.05. This value is a reasonable starting point for most networks but can be adjusted based on the network architecture and specific requirements.</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>Mean:</b> Typically set to 0.0. In some cases, it might be adjusted if you want to center the distribution around a different value.</li>
                                                <li><b>Standard Deviation:</b> Should be chosen based on the scale of the weights needed. For deeper networks, you might need to adjust the standard deviation to ensure that weights are neither too small nor too large.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>Small Networks:</b> In shallow networks, the default values of mean and standard deviation often work well.</li>
                                                <li><b>Deep Networks:</b> In deeper networks, you might need to experiment with different values for <code>stddev</code> or use other initializers like Xavier (Glorot) or He initialization, which are designed to address specific issues in deep networks.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Vanishing and Exploding Gradients:</b> If the weights are initialized with very high or very low values, gradients might vanish or explode during backpropagation, making training difficult.</li>
                                                <li><b>Symmetry Breaking:</b> Proper initialization helps in breaking symmetry, ensuring that neurons learn different features.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>
                                

                            </div>
                        </p>
                    </div>
                </a>
                
                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/02. RandomUniform class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/02. RandomUniform class/random_uniform_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>RandomUniform</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>RandomUniform Initializer in Keras</h1>
                        
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>RandomUniform</code> initializer generates weights by drawing samples from a uniform distribution within a specified range. The formula for the weights <code>W</code> initialized using <code>RandomUniform</code> is:</p>
                                    <div class="equation">
                                        \[
                                        W \sim \text{Uniform}(\text{minval}, \text{maxval})
                                        \]
                                    </div>
                                    <p>where \text{Uniform} denotes a uniform distribution with:</p>
                                    <ul>
                                        <li><b>minval</b>: The lower bound of the uniform distribution.</li>
                                        <li><b>maxval</b>: The upper bound of the uniform distribution.</li>
                                    </ul>
                                </div>
                        
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>RandomUniform</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>Why Use Uniform Distribution?</b> Initializing weights with a uniform distribution helps in breaking symmetry and ensures that weights are distributed within a specific range.</li>
                                                <li><b>Impact on Training:</b> The choice of minval and maxval can affect the performance and convergence of the model. Weights that are too large or too small can lead to slow convergence or instability.</li>
                                            </ul>
                                        </li>
                                        <li><b>Default Values</b>:
                                            <ul>
                                                <li><b>minval:</b> Default is -0.05.</li>
                                                <li><b>maxval:</b> Default is 0.05. These default values are a good starting point but can be adjusted based on the network architecture and specific requirements.</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>minval and maxval:</b> Should be chosen based on the scale of the weights needed. For different network architectures or activation functions, you might need to adjust these values to ensure that weights are neither too small nor too large.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>Small Networks:</b> In shallow networks, the default values of minval and maxval often work well.</li>
                                                <li><b>Deep Networks:</b> In deeper networks, you might need to experiment with different values for minval and maxval or consider other initializers like Xavier (Glorot) or He initialization to address specific issues in deep networks.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Vanishing and Exploding Gradients:</b> If the weights are initialized with very high or very low values, gradients might vanish or explode during backpropagation, making training difficult.</li>
                                                <li><b>Symmetry Breaking:</b> Proper initialization helps in breaking symmetry, ensuring that neurons learn different features.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>
                                </div>
                        </p>
                    </div>
                </a>


                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/03. TruncatedNormal class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/03. TruncatedNormal class/truncated_normal_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>TruncatedNormal</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>TruncatedNormal Initializer in Keras</h1>
                        
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>TruncatedNormal</code> initializer generates weights by drawing samples from a truncated normal distribution. The truncated normal distribution is a normal distribution that has been truncated to lie within a specified range. The formula for the weights <code>W</code> initialized using <code>TruncatedNormal</code> is:</p>
                                    <div class="equation">
                                        <h3>Probability Density Function (PDF)</h3>
                                        \[
                                        f(x) = \frac{\phi\left(\frac{x - \mu}{\sigma}\right)}{\Phi\left(\frac{b - \mu}{\sigma}\right) - \Phi\left(\frac{a - \mu}{\sigma}\right)}
                                        \]
                                        <h3>Cumulative Distribution Function (CDF)</h3>
                                        \[
                                        F(x) = \frac{\Phi\left(\frac{x - \mu}{\sigma}\right) - \Phi\left(\frac{a - \mu}{\sigma}\right)}{\Phi\left(\frac{b - \mu}{\sigma}\right) - \Phi\left(\frac{a - \mu}{\sigma}\right)}
                                        \]
                                    </div>
                                    <p>where:</p>
                                    <ul>
                                        <li> \( \phi \) is the PDF of the standard normal distribution:
                                            \[
                                            \phi(z) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{z^2}{2}}
                                            \]
                                        </li>
                                        <li>\( \Phi \) is the CDF of the standard normal distribution:
                                            \[
                                            \Phi(z) = \int_{-\infty}^z \phi(t) \, dt
                                            \]
                                        </li>
                                        <li>\(  \mu\) is the mean of the original normal distribution.</li>
                                        <li>\(   \sigma\) is the standard deviation of the original normal distribution.</li>
                                        <li>\(  a \)and \( b\) are the lower and upper bounds for truncation.</li>
                                    </ul>
                                </div>
                        
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>TruncatedNormal</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>Why Use Truncated Normal Distribution?</b> The truncated normal distribution helps in avoiding extreme values by truncating the tails of the normal distribution, which can lead to better training stability.</li>
                                                <li><b>Impact on Training:</b> The truncation ensures that the initialized weights are within a reasonable range, which can help in preventing issues such as exploding or vanishing gradients.</li>
                                            </ul>
                                        </li>
                                        <li><b>Default Values</b>:
                                            <ul>
                                                <li><b>mean:</b> Default is 0.0.</li>
                                                <li><b>stddev:</b> Default is 0.05.</li>
                                                <li><b>minval:</b> Default is -2.0 * <code>stddev</code>.</li>
                                                <li><b>maxval:</b> Default is 2.0 * <code>stddev</code>.</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>mean:</b> Typically set to 0.0. In some cases, it might be adjusted based on the specific requirements of the network.</li>
                                                <li><b>stddev:</b> Should be chosen based on the scale of the weights needed. For different network architectures, you might need to adjust the standard deviation to ensure that weights are within a suitable range.</li>
                                                <li><b>minval and maxval:</b> The default values are often reasonable, but you might need to adjust them depending on the network's depth and architecture.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>Small Networks:</b> In shallow networks, the default values for mean and stddev often work well.</li>
                                                <li><b>Deep Networks:</b> In deeper networks, you might need to experiment with different values for mean, stddev, minval, and maxval to ensure proper weight initialization.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Vanishing and Exploding Gradients:</b> Proper initialization helps in avoiding issues related to vanishing and exploding gradients by keeping weights within a reasonable range.</li>
                                                <li><b>Symmetry Breaking:</b> The truncated normal initialization helps in breaking symmetry and ensuring that neurons learn different features.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>
                            </div>
                            
                        </p>
                    </div>
                </a>

                <!-- Add more cards as needed -->
            </div>
        </main>
    </div>
</body>
</html>
