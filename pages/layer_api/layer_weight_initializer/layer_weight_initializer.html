<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Layer Weight Initializers</title>
    <link rel="stylesheet" href="../../../styles.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.2/katex.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.2/katex.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.2/contrib/auto-render.min.js"></script>
  
</head>
<body>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    { left: "$$", right: "$$", display: true },
                    { left: "$", right: "$", display: false }
                ]
            });

            document.querySelectorAll('.show-more-btn').forEach(button => {
                button.addEventListener('click', function(event) {
                    event.stopPropagation(); // Prevents the click event from propagating to the parent <a> element
                    event.preventDefault();  // Prevents the default action of the button, if it has a default action
                    
                    const card = this.closest('.card');
                    const content = card.querySelector('.card-content');
                    const isHidden = content.classList.contains('hidden-content');

                    if (isHidden) {
                        content.classList.remove('hidden-content');
                        this.textContent = "hide";
                    } else {
                        content.classList.add('hidden-content');
                        this.textContent = "show";
                    }
                });
            });
        });
    </script>
    <div class="layout">
        <main class="content-area">
            <h1>Layer Weight Initializers</h1>
            <div class="card-grid">
                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/01. RandomNormal class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/01. RandomNormal class/random_normal_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>RandomNormal</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>RandomNormal Initializer in Keras</h1>
                        
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>RandomNormal</code> initializer generates weights according to the normal distribution defined by the following parameters:</p>
                                    <ul>
                                        <li><b>Mean (μ)</b>: The mean of the normal distribution.</li>
                                        <li><b>Standard Deviation (σ)</b>: The standard deviation of the normal distribution.</li>
                                    </ul>
                                    <p>The formula for the weights <code>W</code> initialized using <code>RandomNormal</code> is:</p>
                                    <div class="equation">
                                        \[
                                        W \sim \mathcal{N} (\text{mean}, \text{stddev}^2)
                                        \]
                                    </div>
                                    <p>where \( \mathcal{N} \)  denotes a normal distribution with:</p>
                                    <ul>
                                        <li><b>Mean (μ)</b>: The average value of the distribution.</li>
                                        <li><b>Variance (σ²)</b>: The square of the standard deviation, representing the spread of the distribution.</li>
                                    </ul>
                                </div>
                        
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>RandomNormal</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>Why Use Normal Distribution?</b> Initializing weights with a normal distribution helps in breaking the symmetry and can improve convergence speed during training.</li>
                                                <li><b>Impact on Training:</b> The choice of mean and standard deviation can affect the performance and convergence of the model. For example, weights that are too large or too small can lead to problems such as vanishing or exploding gradients.</li>
                                            </ul>
                                        </li>
                                        <li><b>Default Values</b>:
                                            <ul>
                                                <li><b>Mean:</b> Default is 0.0.</li>
                                                <li><b>Standard Deviation:</b> Default is 0.05. This value is a reasonable starting point for most networks but can be adjusted based on the network architecture and specific requirements.</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>Mean:</b> Typically set to 0.0. In some cases, it might be adjusted if you want to center the distribution around a different value.</li>
                                                <li><b>Standard Deviation:</b> Should be chosen based on the scale of the weights needed. For deeper networks, you might need to adjust the standard deviation to ensure that weights are neither too small nor too large.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>Small Networks:</b> In shallow networks, the default values of mean and standard deviation often work well.</li>
                                                <li><b>Deep Networks:</b> In deeper networks, you might need to experiment with different values for <code>stddev</code> or use other initializers like Xavier (Glorot) or He initialization, which are designed to address specific issues in deep networks.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Vanishing and Exploding Gradients:</b> If the weights are initialized with very high or very low values, gradients might vanish or explode during backpropagation, making training difficult.</li>
                                                <li><b>Symmetry Breaking:</b> Proper initialization helps in breaking symmetry, ensuring that neurons learn different features.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>
                                

                            </div>
                        </p>
                    </div>
                </a>
                
                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/02. RandomUniform class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/02. RandomUniform class/random_uniform_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>RandomUniform</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>RandomUniform Initializer in Keras</h1>
                        
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>RandomUniform</code> initializer generates weights by drawing samples from a uniform distribution within a specified range. The formula for the weights <code>W</code> initialized using <code>RandomUniform</code> is:</p>
                                    <div class="equation">
                                        \[
                                        W \sim \text{Uniform}(\text{minval}, \text{maxval})
                                        \]
                                    </div>
                                    <p>where \text{Uniform} denotes a uniform distribution with:</p>
                                    <ul>
                                        <li><b>minval</b>: The lower bound of the uniform distribution.</li>
                                        <li><b>maxval</b>: The upper bound of the uniform distribution.</li>
                                    </ul>
                                </div>
                        
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>RandomUniform</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>Why Use Uniform Distribution?</b> Initializing weights with a uniform distribution helps in breaking symmetry and ensures that weights are distributed within a specific range.</li>
                                                <li><b>Impact on Training:</b> The choice of minval and maxval can affect the performance and convergence of the model. Weights that are too large or too small can lead to slow convergence or instability.</li>
                                            </ul>
                                        </li>
                                        <li><b>Default Values</b>:
                                            <ul>
                                                <li><b>minval:</b> Default is -0.05.</li>
                                                <li><b>maxval:</b> Default is 0.05. These default values are a good starting point but can be adjusted based on the network architecture and specific requirements.</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>minval and maxval:</b> Should be chosen based on the scale of the weights needed. For different network architectures or activation functions, you might need to adjust these values to ensure that weights are neither too small nor too large.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>Small Networks:</b> In shallow networks, the default values of minval and maxval often work well.</li>
                                                <li><b>Deep Networks:</b> In deeper networks, you might need to experiment with different values for minval and maxval or consider other initializers like Xavier (Glorot) or He initialization to address specific issues in deep networks.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Vanishing and Exploding Gradients:</b> If the weights are initialized with very high or very low values, gradients might vanish or explode during backpropagation, making training difficult.</li>
                                                <li><b>Symmetry Breaking:</b> Proper initialization helps in breaking symmetry, ensuring that neurons learn different features.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>
                                </div>
                        </p>
                    </div>
                </a>


                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/03. TruncatedNormal class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/03. TruncatedNormal class/truncated_normal_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>TruncatedNormal</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>TruncatedNormal Initializer in Keras</h1>
                        
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>TruncatedNormal</code> initializer generates weights by drawing samples from a truncated normal distribution. The truncated normal distribution is a normal distribution that has been truncated to lie within a specified range. The formula for the weights <code>W</code> initialized using <code>TruncatedNormal</code> is:</p>
                                    <div class="equation">
                                        <h3>Probability Density Function (PDF)</h3>
                                        \[
                                        f(x) = \frac{\phi\left(\frac{x - \mu}{\sigma}\right)}{\Phi\left(\frac{b - \mu}{\sigma}\right) - \Phi\left(\frac{a - \mu}{\sigma}\right)}
                                        \]
                                        <h3>Cumulative Distribution Function (CDF)</h3>
                                        \[
                                        F(x) = \frac{\Phi\left(\frac{x - \mu}{\sigma}\right) - \Phi\left(\frac{a - \mu}{\sigma}\right)}{\Phi\left(\frac{b - \mu}{\sigma}\right) - \Phi\left(\frac{a - \mu}{\sigma}\right)}
                                        \]
                                    </div>
                                    <p>where:</p>
                                    <ul>
                                        <li> \( \phi \) is the PDF of the standard normal distribution:
                                            \[
                                            \phi(z) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{z^2}{2}}
                                            \]
                                        </li>
                                        <li>\( \Phi \) is the CDF of the standard normal distribution:
                                            \[
                                            \Phi(z) = \int_{-\infty}^z \phi(t) \, dt
                                            \]
                                        </li>
                                        <li>\(  \mu\) is the mean of the original normal distribution.</li>
                                        <li>\(   \sigma\) is the standard deviation of the original normal distribution.</li>
                                        <li>\(  a \)and \( b\) are the lower and upper bounds for truncation.</li>
                                    </ul>
                                </div>
                        
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>TruncatedNormal</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>Why Use Truncated Normal Distribution?</b> The truncated normal distribution helps in avoiding extreme values by truncating the tails of the normal distribution, which can lead to better training stability.</li>
                                                <li><b>Impact on Training:</b> The truncation ensures that the initialized weights are within a reasonable range, which can help in preventing issues such as exploding or vanishing gradients.</li>
                                            </ul>
                                        </li>
                                        <li><b>Default Values</b>:
                                            <ul>
                                                <li><b>mean:</b> Default is 0.0.</li>
                                                <li><b>stddev:</b> Default is 0.05.</li>
                                                <li><b>minval:</b> Default is -2.0 * <code>stddev</code>.</li>
                                                <li><b>maxval:</b> Default is 2.0 * <code>stddev</code>.</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>mean:</b> Typically set to 0.0. In some cases, it might be adjusted based on the specific requirements of the network.</li>
                                                <li><b>stddev:</b> Should be chosen based on the scale of the weights needed. For different network architectures, you might need to adjust the standard deviation to ensure that weights are within a suitable range.</li>
                                                <li><b>minval and maxval:</b> The default values are often reasonable, but you might need to adjust them depending on the network's depth and architecture.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>Small Networks:</b> In shallow networks, the default values for mean and stddev often work well.</li>
                                                <li><b>Deep Networks:</b> In deeper networks, you might need to experiment with different values for mean, stddev, minval, and maxval to ensure proper weight initialization.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Vanishing and Exploding Gradients:</b> Proper initialization helps in avoiding issues related to vanishing and exploding gradients by keeping weights within a reasonable range.</li>
                                                <li><b>Symmetry Breaking:</b> The truncated normal initialization helps in breaking symmetry and ensuring that neurons learn different features.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>
                            </div>
                            
                        </p>
                    </div>
                </a>


                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/04. Zeros class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/04. Zeros class/zeros_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>Zeros</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>Zeros Initializer in Keras</h1>
                        
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>Zeros</code> initializer sets all weights to zero. It does not use any distribution or randomness. This initializer is defined by the following formula:</p>
                                    <div class="equation">
                                        \[
                                        W = 0
                                        \]
                                        where \( W \) represents the initialized weight, and it is set to zero.
                                    </div>
                                </div>
                        
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>Zeros</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>Why Use Zeros Initialization?</b> The zeros initializer is used to set all weights to zero. While this might seem simple, it is rarely used in practice for hidden layers because it can lead to issues during training.</li>
                                                <li><b>Impact on Training:</b> Using zeros for weight initialization can cause problems such as symmetry. When all weights are initialized to zero, each neuron in a layer will learn the same features during training, leading to poor model performance.</li>
                                            </ul>
                                        </li>
                                        <li><b>Default Values</b>:
                                            <ul>
                                                <li><b>Value:</b> All weights are initialized to 0.</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>Mean and Variance:</b> There are no parameters to choose as all weights are set to zero.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>Output Layers:</b> The zeros initializer is sometimes used for biases in output layers where zero initialization might be acceptable or required.</li>
                                                <li><b>Hidden Layers:</b> It is generally not recommended to use the zeros initializer for weights in hidden layers due to the issues with symmetry breaking.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Symmetry Problem:</b> Initializing all weights to zero can lead to symmetry problems where neurons in the same layer learn the same features, causing poor model learning.</li>
                                                <li><b>Gradient Flow:</b> Zero initialization can affect the gradient flow during backpropagation, leading to ineffective training.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>
                        
                        </div>
                            
                        </p>
                    </div>
                </a>


                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/05. Ones class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/05. Ones class/ones_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>Ones</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>Ones Initializer in Keras</h1>
                        
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>Ones</code> initializer sets all weights to one. It does not use any distribution or randomness. This initializer is defined by the following formula:</p>
                                    <div class="equation">
                                        \[
                                        W = 1
                                        \]
                                        where \( W \) represents the initialized weight, and it is set to one.
                                    </div>
                                </div>
                        
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>Ones</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>Why Use Ones Initialization?</b> The ones initializer is used to set all weights to one. While this might seem straightforward, it is rarely used for weight initialization in practice due to potential issues during training.</li>
                                                <li><b>Impact on Training:</b> Using ones for weight initialization can cause problems similar to those with zeros initialization, such as symmetry and gradient issues.</li>
                                            </ul>
                                        </li>
                                        <li><b>Default Values</b>:
                                            <ul>
                                                <li><b>Value:</b> All weights are initialized to 1.</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>Value:</b> There are no parameters to choose, as all weights are set to one.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>Output Layers:</b> The ones initializer is sometimes used for biases in specific scenarios where initializing with ones might be acceptable or desired.</li>
                                                <li><b>Hidden Layers:</b> It is generally not recommended to use the ones initializer for weights in hidden layers due to potential symmetry and gradient issues.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Symmetry Problem:</b> Initializing all weights to one can lead to symmetry issues where neurons in the same layer learn the same features, leading to ineffective learning.</li>
                                                <li><b>Gradient Flow:</b> Initializing weights with ones can affect the gradient flow during backpropagation, which may hinder effective training.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>
                            </div>
                            
                        </p>
                    </div>
                </a>

                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/06. GlorotNormal class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/06. GlorotNormal class/glorot_normal_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>GlorotNormal</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>GlorotNormal Initializer in Keras</h1>
                        
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>GlorotNormal</code> initializer, also known as Xavier Normal initializer, is designed to keep the variance of activations and gradients approximately the same across all layers. This helps in avoiding vanishing and exploding gradients problems. The weights are drawn from a normal distribution with a mean of 0 and a variance calculated based on the number of input and output units of the layer.</p>
                                    <div class="equation">
                                        The variance of the normal distribution is given by:
                                        \[
                                        \text{Var}(W) = \frac{2}{\text{input_units} + \text{output_units}}
                                        \]
                                        where \( W \) represents the weights initialized, and \( \text{input_units} \) and \( \text{output_units} \) are the number of input and output units for the layer, respectively.
                                    </div>
                                    <div class="equation">
                                        Thus, the weights are drawn from:
                                        \[
                                        W \sim \mathcal{N}(0, \text{Var}(W))
                                        \]
                                        where \( \mathcal{N} \) denotes a normal distribution with mean 0 and the calculated variance.
                                    </div>
                                </div>
                        
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>GlorotNormal</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>Why Use Glorot Normal Initialization?</b> This initializer is effective for deep neural networks as it helps in maintaining a stable variance of activations and gradients throughout the network. It is particularly useful for avoiding problems such as vanishing and exploding gradients.</li>
                                                <li><b>Impact on Training:</b> By initializing the weights in a way that balances the variance, Glorot Normal can help in improving the convergence speed and stability of training.</li>
                                            </ul>
                                        </li>
                                        <li><b>Default Values</b>:
                                            <ul>
                                                <li><b>Mean:</b> The weights are initialized with a mean of 0.</li>
                                                <li><b>Variance:</b> Calculated as \( \frac{2}{\text{input_units} + \text{output_units}} \).</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>Mean:</b> Always set to 0 in this initializer.</li>
                                                <li><b>Variance:</b> Automatically determined based on the input and output units of the layer.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>Deep Networks:</b> Suitable for deep networks where maintaining stable gradients and activations is crucial.</li>
                                                <li><b>Activation Functions:</b> Works well with activation functions like ReLU, where initialization can significantly impact performance.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Layer Variance:</b> Ensure that the variance calculation is appropriate for your network's architecture to avoid issues with gradient flow.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>
                        
                        </div>
                        </p>
                    </div>
                </a>

                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/07. GlorotUniform class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/07. GlorotUniform class/glorot_uniform_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>GlorotUniform</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            
                        </p>
                    </div>
                </a>


                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/08. HeNormal class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/08. HeNormal class/he_normal_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>HeNormal</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            
                        </p>
                    </div>
                </a>

                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/09. HeUniform class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/09. HeUniform class/he_uniform_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>HeUniform</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            
                        </p>
                    </div>
                </a>

                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/10. Orthogonal class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/10. Orthogonal class/orthogonal_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>Orthogonal</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            
                        </p>
                    </div>
                </a>

                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/11. Constant class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/11. Constant class/constant_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>Constant</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            
                        </p>
                    </div>
                </a>

                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/12. VarianceScaling class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/12. VarianceScaling class/variance_scaling_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>VarianceScaling</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            
                        </p>
                    </div>
                </a>

                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/13. LecunNormal class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/13. LecunNormal class/lecun_normal_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>LecunNormal</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            
                        </p>
                    </div>
                </a>

                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/14. LecunUniform class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/14. LecunUniform class/lecun_uniform_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>LecunUniform</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            
                        </p>
                    </div>
                </a>

                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/15. IdentityInitializer class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/15. IdentityInitializer class/identity_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>IdentityInitializer</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            
                        </p>
                    </div>
                </a>
                
                <!-- Add more cards as needed -->
            </div>
        </main>
    </div>
</body>
</html>
