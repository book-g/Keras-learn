<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Layer Weight Initializers</title>
    <link rel="stylesheet" href="../../../styles.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.2/katex.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.2/katex.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.2/contrib/auto-render.min.js"></script>
  
</head>
<body>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    { left: "$$", right: "$$", display: true },
                    { left: "$", right: "$", display: false }
                ]
            });

            document.querySelectorAll('.show-more-btn').forEach(button => {
                button.addEventListener('click', function(event) {
                    event.stopPropagation(); // Prevents the click event from propagating to the parent <a> element
                    event.preventDefault();  // Prevents the default action of the button, if it has a default action
                    
                    const card = this.closest('.card');
                    const content = card.querySelector('.card-content');
                    const isHidden = content.classList.contains('hidden-content');

                    if (isHidden) {
                        content.classList.remove('hidden-content');
                        this.textContent = "hide";
                    } else {
                        content.classList.add('hidden-content');
                        this.textContent = "show";
                    }
                });
            });
        });
    </script>
    <div class="layout">
        <main class="content-area">
            <h1>Layer Weight Initializers</h1>
            <p>Layer weight initializers in Keras 3 are essential tools for setting the starting point of a model's learning process. By choosing the right initializer, you can improve the model's performance, ensure stable training, and avoid issues like vanishing or exploding gradients. Understanding and applying these initializers is key to building effective deep learning models.</p>
            <div class="card-grid">
                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/01. RandomNormal class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/01. RandomNormal class/random_normal_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>RandomNormal</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>RandomNormal Initializer in Keras</h1>
                        
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>RandomNormal</code> initializer generates weights according to the normal distribution defined by the following parameters:</p>
                                    <ul>
                                        <li><b>Mean (μ)</b>: The mean of the normal distribution.</li>
                                        <li><b>Standard Deviation (σ)</b>: The standard deviation of the normal distribution.</li>
                                    </ul>
                                    <p>The formula for the weights <code>W</code> initialized using <code>RandomNormal</code> is:</p>
                                    <div class="equation">
                                        \[
                                        W \sim \mathcal{N} (\text{mean}, \text{stddev}^2)
                                        \]
                                    </div>
                                    <p>where \( \mathcal{N} \)  denotes a normal distribution with:</p>
                                    <ul>
                                        <li><b>Mean (μ)</b>: The average value of the distribution.</li>
                                        <li><b>Variance (σ²)</b>: The square of the standard deviation, representing the spread of the distribution.</li>
                                    </ul>
                                </div>
                        
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>RandomNormal</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>Why Use Normal Distribution?</b> Initializing weights with a normal distribution helps in breaking the symmetry and can improve convergence speed during training.</li>
                                                <li><b>Impact on Training:</b> The choice of mean and standard deviation can affect the performance and convergence of the model. For example, weights that are too large or too small can lead to problems such as vanishing or exploding gradients.</li>
                                            </ul>
                                        </li>
                                        <li><b>Default Values</b>:
                                            <ul>
                                                <li><b>Mean:</b> Default is 0.0.</li>
                                                <li><b>Standard Deviation:</b> Default is 0.05. This value is a reasonable starting point for most networks but can be adjusted based on the network architecture and specific requirements.</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>Mean:</b> Typically set to 0.0. In some cases, it might be adjusted if you want to center the distribution around a different value.</li>
                                                <li><b>Standard Deviation:</b> Should be chosen based on the scale of the weights needed. For deeper networks, you might need to adjust the standard deviation to ensure that weights are neither too small nor too large.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>Small Networks:</b> In shallow networks, the default values of mean and standard deviation often work well.</li>
                                                <li><b>Deep Networks:</b> In deeper networks, you might need to experiment with different values for <code>stddev</code> or use other initializers like Xavier (Glorot) or He initialization, which are designed to address specific issues in deep networks.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Vanishing and Exploding Gradients:</b> If the weights are initialized with very high or very low values, gradients might vanish or explode during backpropagation, making training difficult.</li>
                                                <li><b>Symmetry Breaking:</b> Proper initialization helps in breaking symmetry, ensuring that neurons learn different features.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>
                                

                            </div>
                        </p>
                    </div>
                </a>
                
                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/02. RandomUniform class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/02. RandomUniform class/random_uniform_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>RandomUniform</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>RandomUniform Initializer in Keras</h1>
                        
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>RandomUniform</code> initializer generates weights by drawing samples from a uniform distribution within a specified range. The formula for the weights <code>W</code> initialized using <code>RandomUniform</code> is:</p>
                                    <div class="equation">
                                        \[
                                        W \sim \text{Uniform}(\text{minval}, \text{maxval})
                                        \]
                                    </div>
                                    <p>where \text{Uniform} denotes a uniform distribution with:</p>
                                    <ul>
                                        <li><b>minval</b>: The lower bound of the uniform distribution.</li>
                                        <li><b>maxval</b>: The upper bound of the uniform distribution.</li>
                                    </ul>
                                </div>
                        
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>RandomUniform</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>Why Use Uniform Distribution?</b> Initializing weights with a uniform distribution helps in breaking symmetry and ensures that weights are distributed within a specific range.</li>
                                                <li><b>Impact on Training:</b> The choice of minval and maxval can affect the performance and convergence of the model. Weights that are too large or too small can lead to slow convergence or instability.</li>
                                            </ul>
                                        </li>
                                        <li><b>Default Values</b>:
                                            <ul>
                                                <li><b>minval:</b> Default is -0.05.</li>
                                                <li><b>maxval:</b> Default is 0.05. These default values are a good starting point but can be adjusted based on the network architecture and specific requirements.</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>minval and maxval:</b> Should be chosen based on the scale of the weights needed. For different network architectures or activation functions, you might need to adjust these values to ensure that weights are neither too small nor too large.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>Small Networks:</b> In shallow networks, the default values of minval and maxval often work well.</li>
                                                <li><b>Deep Networks:</b> In deeper networks, you might need to experiment with different values for minval and maxval or consider other initializers like Xavier (Glorot) or He initialization to address specific issues in deep networks.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Vanishing and Exploding Gradients:</b> If the weights are initialized with very high or very low values, gradients might vanish or explode during backpropagation, making training difficult.</li>
                                                <li><b>Symmetry Breaking:</b> Proper initialization helps in breaking symmetry, ensuring that neurons learn different features.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>
                                </div>
                        </p>
                    </div>
                </a>


                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/03. TruncatedNormal class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/03. TruncatedNormal class/truncated_normal_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>TruncatedNormal</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>TruncatedNormal Initializer in Keras</h1>
                        
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>TruncatedNormal</code> initializer generates weights by drawing samples from a truncated normal distribution. The truncated normal distribution is a normal distribution that has been truncated to lie within a specified range. The formula for the weights <code>W</code> initialized using <code>TruncatedNormal</code> is:</p>
                                    <div class="equation">
                                        <h3>Probability Density Function (PDF)</h3>
                                        \[
                                        f(x) = \frac{\phi\left(\frac{x - \mu}{\sigma}\right)}{\Phi\left(\frac{b - \mu}{\sigma}\right) - \Phi\left(\frac{a - \mu}{\sigma}\right)}
                                        \]
                                        <h3>Cumulative Distribution Function (CDF)</h3>
                                        \[
                                        F(x) = \frac{\Phi\left(\frac{x - \mu}{\sigma}\right) - \Phi\left(\frac{a - \mu}{\sigma}\right)}{\Phi\left(\frac{b - \mu}{\sigma}\right) - \Phi\left(\frac{a - \mu}{\sigma}\right)}
                                        \]
                                    </div>
                                    <p>where:</p>
                                    <ul>
                                        <li> \( \phi \) is the PDF of the standard normal distribution:
                                            \[
                                            \phi(z) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{z^2}{2}}
                                            \]
                                        </li>
                                        <li>\( \Phi \) is the CDF of the standard normal distribution:
                                            \[
                                            \Phi(z) = \int_{-\infty}^z \phi(t) \, dt
                                            \]
                                        </li>
                                        <li>\(  \mu\) is the mean of the original normal distribution.</li>
                                        <li>\(   \sigma\) is the standard deviation of the original normal distribution.</li>
                                        <li>\(  a \)and \( b\) are the lower and upper bounds for truncation.</li>
                                    </ul>
                                </div>
                        
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>TruncatedNormal</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>Why Use Truncated Normal Distribution?</b> The truncated normal distribution helps in avoiding extreme values by truncating the tails of the normal distribution, which can lead to better training stability.</li>
                                                <li><b>Impact on Training:</b> The truncation ensures that the initialized weights are within a reasonable range, which can help in preventing issues such as exploding or vanishing gradients.</li>
                                            </ul>
                                        </li>
                                        <li><b>Default Values</b>:
                                            <ul>
                                                <li><b>mean:</b> Default is 0.0.</li>
                                                <li><b>stddev:</b> Default is 0.05.</li>
                                                <li><b>minval:</b> Default is -2.0 * <code>stddev</code>.</li>
                                                <li><b>maxval:</b> Default is 2.0 * <code>stddev</code>.</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>mean:</b> Typically set to 0.0. In some cases, it might be adjusted based on the specific requirements of the network.</li>
                                                <li><b>stddev:</b> Should be chosen based on the scale of the weights needed. For different network architectures, you might need to adjust the standard deviation to ensure that weights are within a suitable range.</li>
                                                <li><b>minval and maxval:</b> The default values are often reasonable, but you might need to adjust them depending on the network's depth and architecture.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>Small Networks:</b> In shallow networks, the default values for mean and stddev often work well.</li>
                                                <li><b>Deep Networks:</b> In deeper networks, you might need to experiment with different values for mean, stddev, minval, and maxval to ensure proper weight initialization.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Vanishing and Exploding Gradients:</b> Proper initialization helps in avoiding issues related to vanishing and exploding gradients by keeping weights within a reasonable range.</li>
                                                <li><b>Symmetry Breaking:</b> The truncated normal initialization helps in breaking symmetry and ensuring that neurons learn different features.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>
                            </div>
                            
                        </p>
                    </div>
                </a>


                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/04. Zeros class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/04. Zeros class/zeros_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>Zeros</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>Zeros Initializer in Keras</h1>
                        
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>Zeros</code> initializer sets all weights to zero. It does not use any distribution or randomness. This initializer is defined by the following formula:</p>
                                    <div class="equation">
                                        \[
                                        W = 0
                                        \]
                                        where \( W \) represents the initialized weight, and it is set to zero.
                                    </div>
                                </div>
                        
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>Zeros</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>Why Use Zeros Initialization?</b> The zeros initializer is used to set all weights to zero. While this might seem simple, it is rarely used in practice for hidden layers because it can lead to issues during training.</li>
                                                <li><b>Impact on Training:</b> Using zeros for weight initialization can cause problems such as symmetry. When all weights are initialized to zero, each neuron in a layer will learn the same features during training, leading to poor model performance.</li>
                                            </ul>
                                        </li>
                                        <li><b>Default Values</b>:
                                            <ul>
                                                <li><b>Value:</b> All weights are initialized to 0.</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>Mean and Variance:</b> There are no parameters to choose as all weights are set to zero.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>Output Layers:</b> The zeros initializer is sometimes used for biases in output layers where zero initialization might be acceptable or required.</li>
                                                <li><b>Hidden Layers:</b> It is generally not recommended to use the zeros initializer for weights in hidden layers due to the issues with symmetry breaking.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Symmetry Problem:</b> Initializing all weights to zero can lead to symmetry problems where neurons in the same layer learn the same features, causing poor model learning.</li>
                                                <li><b>Gradient Flow:</b> Zero initialization can affect the gradient flow during backpropagation, leading to ineffective training.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>
                        
                        </div>
                            
                        </p>
                    </div>
                </a>


                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/05. Ones class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/05. Ones class/ones_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>Ones</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>Ones Initializer in Keras</h1>
                        
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>Ones</code> initializer sets all weights to one. It does not use any distribution or randomness. This initializer is defined by the following formula:</p>
                                    <div class="equation">
                                        \[
                                        W = 1
                                        \]
                                        where \( W \) represents the initialized weight, and it is set to one.
                                    </div>
                                </div>
                        
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>Ones</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>Why Use Ones Initialization?</b> The ones initializer is used to set all weights to one. While this might seem straightforward, it is rarely used for weight initialization in practice due to potential issues during training.</li>
                                                <li><b>Impact on Training:</b> Using ones for weight initialization can cause problems similar to those with zeros initialization, such as symmetry and gradient issues.</li>
                                            </ul>
                                        </li>
                                        <li><b>Default Values</b>:
                                            <ul>
                                                <li><b>Value:</b> All weights are initialized to 1.</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>Value:</b> There are no parameters to choose, as all weights are set to one.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>Output Layers:</b> The ones initializer is sometimes used for biases in specific scenarios where initializing with ones might be acceptable or desired.</li>
                                                <li><b>Hidden Layers:</b> It is generally not recommended to use the ones initializer for weights in hidden layers due to potential symmetry and gradient issues.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Symmetry Problem:</b> Initializing all weights to one can lead to symmetry issues where neurons in the same layer learn the same features, leading to ineffective learning.</li>
                                                <li><b>Gradient Flow:</b> Initializing weights with ones can affect the gradient flow during backpropagation, which may hinder effective training.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>
                            </div>
                            
                        </p>
                    </div>
                </a>

                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/06. GlorotNormal class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/06. GlorotNormal class/glorot_normal_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>GlorotNormal</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>GlorotNormal Initializer in Keras</h1>
                        
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>GlorotNormal</code> initializer, also known as Xavier Normal initializer, is designed to keep the variance of activations and gradients approximately the same across all layers. This helps in avoiding vanishing and exploding gradients problems. The weights are drawn from a normal distribution with a mean of 0 and a variance calculated based on the number of input and output units of the layer.</p>
                                    <div class="equation">
                                        The variance of the normal distribution is given by:
                                        \[
                                        \text{Var}(W) = \frac{2}{\text{input_units} + \text{output_units}}
                                        \]
                                        where \( W \) represents the weights initialized, and \( \text{input_units} \) and \( \text{output_units} \) are the number of input and output units for the layer, respectively.
                                    </div>
                                    <div class="equation">
                                        Thus, the weights are drawn from:
                                        \[
                                        W \sim \mathcal{N}(0, \text{Var}(W))
                                        \]
                                        where \( \mathcal{N} \) denotes a normal distribution with mean 0 and the calculated variance.
                                    </div>
                                </div>
                        
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>GlorotNormal</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>Why Use Glorot Normal Initialization?</b> This initializer is effective for deep neural networks as it helps in maintaining a stable variance of activations and gradients throughout the network. It is particularly useful for avoiding problems such as vanishing and exploding gradients.</li>
                                                <li><b>Impact on Training:</b> By initializing the weights in a way that balances the variance, Glorot Normal can help in improving the convergence speed and stability of training.</li>
                                            </ul>
                                        </li>
                                        <li><b>Default Values</b>:
                                            <ul>
                                                <li><b>Mean:</b> The weights are initialized with a mean of 0.</li>
                                                <li><b>Variance:</b> Calculated as \( \frac{2}{\text{input_units} + \text{output_units}} \).</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>Mean:</b> Always set to 0 in this initializer.</li>
                                                <li><b>Variance:</b> Automatically determined based on the input and output units of the layer.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>Deep Networks:</b> Suitable for deep networks where maintaining stable gradients and activations is crucial.</li>
                                                <li><b>Activation Functions:</b> Works well with activation functions like ReLU, where initialization can significantly impact performance.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Layer Variance:</b> Ensure that the variance calculation is appropriate for your network's architecture to avoid issues with gradient flow.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>
                        
                        </div>
                        </p>
                    </div>
                </a>

                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/07. GlorotUniform class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/07. GlorotUniform class/glorot_uniform_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>GlorotUniform</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>GlorotUniform Initializer in Keras</h1>
                        
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>GlorotUniform</code> initializer, also known as Xavier Uniform initializer, is designed to maintain the variance of activations across layers by drawing weights from a uniform distribution. This helps in avoiding issues such as vanishing or exploding gradients.</p>
                                    <div class="equation">
                                        The weights are drawn from a uniform distribution within the range:
                                        \[
                                        \text{Uniform} \left(- \sqrt{\frac{6}{\text{input_units} + \text{output_units}}}, \sqrt{\frac{6}{\text{input_units} + \text{output_units}}}\right)
                                        \]
                                        where \( \text{input_units} \) and \( \text{output_units} \) are the number of input and output units for the layer, respectively.
                                    </div>
                                </div>
                        
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>GlorotUniform</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>Why Use Glorot Uniform Initialization?</b> This initializer helps maintain a stable variance of activations and gradients throughout the network, which is crucial for deep networks to avoid vanishing or exploding gradient problems.</li>
                                                <li><b>Impact on Training:</b> By initializing the weights in a balanced manner, Glorot Uniform can improve the convergence speed and overall stability of the training process.</li>
                                            </ul>
                                        </li>
                                        <li><b>Default Values</b>:
                                            <ul>
                                                <li><b>Range:</b> The weights are drawn from a uniform distribution within the range specified by the formula above.</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>Range:</b> Automatically determined based on the input and output units of the layer.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>Deep Networks:</b> Suitable for deep networks where maintaining balanced gradients and activations is important.</li>
                                                <li><b>Activation Functions:</b> Works well with activation functions like ReLU and sigmoid, which can benefit from balanced initialization.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Layer Variance:</b> Ensure that the initialization range is appropriate for your network's architecture to avoid issues with gradient flow.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div></div>
                        </p>
                    </div>
                </a>


                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/08. HeNormal class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/08. HeNormal class/he_normal_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>HeNormal</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>HeNormal Initializer in Keras</h1>
                        
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>HeNormal</code> initializer, also known as He Initialization, is designed to address the issue of vanishing and exploding gradients in deep neural networks by initializing weights with values drawn from a normal distribution. It is particularly effective with ReLU activation functions.</p>
                                    <div class="equation">
                                        The weights are drawn from a normal distribution with:
                                        \[
                                        W \sim \mathcal{N}(0, \frac{2}{\text{input_units}})
                                        \]
                                        where \( \mathcal{N} \) denotes a normal distribution with:
                                        - **Mean**: \( 0 \)
                                        - **Variance**: \( \frac{2}{\text{input_units}} \)
                        
                                        Here, \( \text{input_units} \) is the number of input units in the layer.
                                    </div>
                                </div>
                        
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>HeNormal</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>Why Use He Normal Initialization?</b> This initializer helps mitigate the vanishing and exploding gradient problems that can occur in deep networks, especially when using ReLU activation functions.</li>
                                                <li><b>Impact on Training:</b> By initializing weights with a higher variance, He Normal helps maintain a healthy gradient flow through the network, which can improve convergence and stability during training.</li>
                                            </ul>
                                        </li>
                                        <li><b>Default Values</b>:
                                            <ul>
                                                <li><b>Mean:</b> 0</li>
                                                <li><b>Variance:</b> \( \frac{2}{\text{input_units}} \)</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>Variance:</b> The variance is automatically determined based on the number of input units. This ensures that the weights are initialized in a way that supports effective training.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>ReLU Activation:</b> Works well with ReLU and its variants (e.g., Leaky ReLU, Parametric ReLU) because it compensates for the non-linearities introduced by these activations.</li>
                                                <li><b>Deep Networks:</b> Particularly beneficial in very deep networks where gradients might otherwise vanish or explode.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Layer Variance:</b> Ensure that the initializer is used appropriately with activation functions that benefit from it. For other types of activations, different initializers might be more suitable.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>
                        </div>
                            
                        </p>
                    </div>
                </a>

                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/09. HeUniform class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/09. HeUniform class/he_uniform_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>HeUniform</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>HeUniform Initializer in Keras</h1>
                        
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>HeUniform</code> initializer, also known as He Initialization with Uniform Distribution, is designed to address the issue of vanishing and exploding gradients in deep neural networks by initializing weights with values drawn from a uniform distribution. It is particularly effective with ReLU activation functions.</p>
                                    <div class="equation">
                                        The weights are drawn from a uniform distribution within the range:
                                        \[
                                        W \sim \text{Uniform}\left(-\sqrt{\frac{6}{\text{input_units}}}, \sqrt{\frac{6}{\text{input_units}}}\right)
                                        \]
                                        where \( \text{Uniform}(a, b) \) denotes a uniform distribution between \( a \) and \( b \), with:
                                        - **Lower Bound (a)**: \(-\sqrt{\frac{6}{\text{input_units}}}\)
                                        - **Upper Bound (b)**: \(\sqrt{\frac{6}{\text{input_units}}}\)
                        
                                        Here, \( \text{input_units} \) is the number of input units in the layer.
                                    </div>
                                </div>
                        
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>HeUniform</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>Why Use He Uniform Initialization?</b> This initializer helps mitigate the vanishing and exploding gradient problems that can occur in deep networks, especially when using ReLU activation functions, by maintaining a variance that scales with the number of input units.</li>
                                                <li><b>Impact on Training:</b> By initializing weights with values drawn from a uniform distribution within a specific range, He Uniform ensures that the weights are neither too large nor too small, which helps in effective training and convergence.</li>
                                            </ul>
                                        </li>
                                        <li><b>Default Values</b>:
                                            <ul>
                                                <li><b>Lower Bound:</b> \(-\sqrt{\frac{6}{\text{input_units}}}\)</li>
                                                <li><b>Upper Bound:</b> \(\sqrt{\frac{6}{\text{input_units}}}\)</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>Range:</b> The range is automatically determined based on the number of input units. This ensures that the weights are initialized within a suitable range to support effective training.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>ReLU Activation:</b> Works well with ReLU and its variants (e.g., Leaky ReLU, Parametric ReLU) because it compensates for the non-linearities introduced by these activations.</li>
                                                <li><b>Deep Networks:</b> Particularly beneficial in very deep networks where gradients might otherwise vanish or explode.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Layer Variance:</b> Ensure that the initializer is used appropriately with activation functions that benefit from it. For other types of activations, different initializers might be more suitable.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div></div>
                        </p>
                    </div>
                </a>

                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/10. Orthogonal class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/10. Orthogonal class/orthogonal_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>Orthogonal</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>Orthogonal Initializer in Keras</h1>
                        
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>Orthogonal</code> initializer is designed to initialize weights such that the weight matrix is orthogonal. This means that the columns (or rows) of the weight matrix are orthogonal unit vectors. An orthogonal matrix \( W \) satisfies the property:</p>
                                    <div class="equation">
                                        \[
                                        W^T W = W W^T = I
                                        \]
                                        where \( W^T \) is the transpose of \( W \), and \( I \) is the identity matrix. This ensures that the weight matrix is orthogonal, which can help in preserving the norms of gradients throughout the training process.
                                    </div>
                                </div>
                        
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>Orthogonal</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>Why Use Orthogonal Initialization?</b> Orthogonal initialization helps to maintain the stability of gradients throughout the network. It is especially beneficial in deep networks where gradients might otherwise vanish or explode.</li>
                                                <li><b>Impact on Training:</b> Orthogonal matrices help preserve the variance of activations and gradients, which can lead to more stable and faster convergence during training.</li>
                                            </ul>
                                        </li>
                                        <li><b>Default Values</b>:
                                            <ul>
                                                <li><b>Gain:</b> The default gain value is 1.0. This can be adjusted to scale the weights if necessary.</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>Gain:</b> The gain parameter scales the orthogonal matrix. It can be adjusted based on the activation function used. For example, for ReLU activations, a gain of \(\sqrt{2}\) might be used.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>Deep Networks:</b> Particularly useful in deep networks where maintaining gradient stability is critical.</li>
                                                <li><b>Activation Functions:</b> Works well with a variety of activation functions, including ReLU and its variants.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Matrix Size:</b> The initializer is generally applied to layers where the weight matrix is square. For non-square matrices, the orthogonality is preserved as much as possible but may not be perfect.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>
                        </div>
                            
                        </p>
                    </div>
                </a>

                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/11. Constant class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/11. Constant class/constant_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>Constant</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>Constant Initializer in Keras</h1>
                        
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>Constant</code> initializer sets all the weights in a layer to a constant value. The formula for the weights \( W \) initialized using <code>Constant</code> is:</p>
                                    <div class="equation">
                                        \[
                                        W = c
                                        \]
                                        where \( c \) is the constant value specified during initialization.
                                    </div>
                                </div>
                        
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>Constant</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>Why Use Constant Initialization?</b> Setting weights to a constant value might be useful in specific scenarios where you need uniform initial values. For example, initializing biases to a small constant value to prevent dead neurons in the early stages of training.</li>
                                                <li><b>Impact on Training:</b> Initializing weights with a constant value may not be ideal for most layers, as it doesn’t help in breaking symmetry or introducing variability. It can lead to problems like neurons learning the same features if all weights start with the same value.</li>
                                            </ul>
                                        </li>
                                        <li><b>Default Values</b>:
                                            <ul>
                                                <li><b>Value:</b> The default value is 0.0. This value can be adjusted according to the needs of the model.</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>Value:</b> The constant value should be chosen carefully. For instance, setting biases to a small positive value can help in cases where neurons might be inactive initially.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>Bias Initialization:</b> Commonly used for initializing biases, especially if you want to start with a non-zero constant value.</li>
                                                <li><b>Special Cases:</b> Can be used in custom layers or situations where specific constant initialization is required.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Symmetry Breaking:</b> Using the same constant value for all weights might not help in breaking symmetry in layers, potentially leading to suboptimal learning.</li>
                                                <li><b>Gradient Flow:</b> If used for weights, constant initialization might hinder gradient flow and affect the convergence of the network.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>    
                        </div>
                            
                        </p>
                    </div>
                </a>

                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/12. VarianceScaling class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/12. VarianceScaling class/variance_scaling_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>VarianceScaling</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>VarianceScaling Initializer in Keras</h1>
                        
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>VarianceScaling</code> initializer scales the variance of the weights according to the number of input units in the layer. The formula for the weights \( W \) initialized using <code>VarianceScaling</code> is:</p>
                                    <div class="equation">
                                        \[
                                        W \sim \text{Uniform} \left(-\sqrt{\frac{3}{\text{scale}}}, \sqrt{\frac{3}{\text{scale}}}\right)
                                        \]
                                        or
                                        \[
                                        W \sim \text{Normal} \left(0, \frac{\text{scale}}{\text{fan_in}}\right)
                                        \]
                                        where:
                                        <ul>
                                            <li><b>scale:</b> A scaling factor, typically 1.0, 2.0, or 3.0 depending on the mode.</li>
                                            <li><b>fan_in:</b> The number of input units in the weight tensor.</li>
                                            <li><b>fan_out:</b> The number of output units in the weight tensor.</li>
                                        </ul>
                                    </div>
                                </div>
                        
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>VarianceScaling</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>Why Use Variance Scaling?</b> This initializer helps to maintain the variance of activations across layers, addressing issues like vanishing or exploding gradients by scaling the weights based on the number of input units.</li>
                                                <li><b>Impact on Training:</b> By keeping the variance of activations in check, it can lead to better training stability and faster convergence.</li>
                                            </ul>
                                        </li>
                                        <li><b>Default Values</b>:
                                            <ul>
                                                <li><b>Scale:</b> Default is 1.0. This value can be adjusted depending on the specific needs of the network.</li>
                                                <li><b>Mode:</b> Can be set to 'fan_in', 'fan_out', or 'fan_avg'.</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>Scale:</b> Choose based on the mode and type of activation function used. For ReLU activations, a scale of 2.0 is often recommended (He initialization).</li>
                                                <li><b>Mode:</b> 'fan_in' is often used to scale based on input units, 'fan_out' for output units, and 'fan_avg' for a balance of both.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>ReLU Activations:</b> For layers with ReLU activations, using 'fan_in' with a scale of 2.0 (He initialization) is effective.</li>
                                                <li><b>Custom Initializations:</b> Useful for custom initializers where maintaining variance is crucial.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Choosing the Wrong Mode:</b> Incorrectly choosing between 'fan_in', 'fan_out', and 'fan_avg' can lead to suboptimal performance.</li>
                                                <li><b>Over-Scaling:</b> Excessive scaling might cause instability in training, especially for deep networks.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>
                        
                        </div>
                        </p>
                    </div>
                </a>

                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/13. LecunNormal class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/13. LecunNormal class/lecun_normal_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>LecunNormal</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>LeCunNormal Initializer in Keras</h1>
                        
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>LeCunNormal</code> initializer scales weights according to the number of input units using a normal distribution. The formula for the weights \( W \) initialized using <code>LeCunNormal</code> is:</p>
                                    <div class="equation">
                                        \[
                                        W \sim \mathcal{N}\left(0, \frac{1}{\text{fan_in}}\right)
                                        \]
                                        where:
                                        <ul>
                                            <li><b>\(\mathcal{N}\)</b> denotes a normal distribution.</li>
                                            <li><b>fan_in</b> is the number of input units in the weight tensor.</li>
                                        </ul>
                                    </div>
                                </div>
                        
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>LeCunNormal</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>Why Use Normal Distribution?</b> The LeCunNormal initializer helps in initializing weights with a variance that maintains stable gradients and improves training efficiency. It is particularly well-suited for activation functions like ReLU and its variants.</li>
                                                <li><b>Impact on Training:</b> This initializer can lead to better convergence and reduced training times compared to other initializers, especially in deep networks.</li>
                                            </ul>
                                        </li>
                                        <li><b>Default Values</b>:
                                            <ul>
                                                <li><b>Mean:</b> Default is 0.0.</li>
                                                <li><b>Standard Deviation:</b> Scaled as \( \frac{1}{\sqrt{\text{fan_in}}} \), where fan_in is the number of input units.</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>Mean:</b> Typically set to 0.0.</li>
                                                <li><b>Standard Deviation:</b> Automatically adjusted based on the number of input units, ensuring appropriate variance.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>ReLU Activations:</b> Effective for layers with ReLU or its variants, helping to avoid issues like vanishing gradients.</li>
                                                <li><b>Deep Networks:</b> Useful in deep networks to maintain stable variance of activations across layers.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Compatibility:</b> Not ideal for all activation functions; other initializers may be more suitable for non-ReLU activations.</li>
                                                <li><b>Over-Sensitivity:</b> The performance can be sensitive to the choice of activation functions and network architecture.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>
                        
                        </div>
                        </p>
                    </div>
                </a>

                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/14. LecunUniform class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/14. LecunUniform class/lecun_uniform_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>LecunUniform</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>LeCunUniform Initializer in Keras</h1>
                        
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>LeCunUniform</code> initializer generates weights according to a uniform distribution scaled by the number of input units. The formula for the weights \( W \) initialized using <code>LeCunUniform</code> is:</p>
                                    <div class="equation">
                                        \[
                                        W \sim \text{Uniform}\left(-\sqrt{\frac{3}{\text{fan_in}}}, \sqrt{\frac{3}{\text{fan_in}}}\right)
                                        \]
                                        where:
                                        <ul>
                                            <li><b>\(\text{Uniform}\)</b> denotes a uniform distribution.</li>
                                            <li><b>fan_in</b> is the number of input units in the weight tensor.</li>
                                        </ul>
                                    </div>
                                </div>
                        
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>LeCunUniform</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>Why Use Uniform Distribution?</b> The LeCunUniform initializer scales weights uniformly across a range that depends on the number of input units, which can help in maintaining stable activations and gradients. It is particularly suited for layers with ReLU and similar activation functions.</li>
                                                <li><b>Impact on Training:</b> This initializer helps prevent issues like vanishing or exploding gradients, leading to more stable and faster training.</li>
                                            </ul>
                                        </li>
                                        <li><b>Default Values</b>:
                                            <ul>
                                                <li><b>Lower Bound:</b> \(-\sqrt{\frac{3}{\text{fan_in}}}\)</li>
                                                <li><b>Upper Bound:</b> \(\sqrt{\frac{3}{\text{fan_in}}}\)</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>Range:</b> The range of the uniform distribution is automatically scaled based on the number of input units, ensuring weights are initialized within an appropriate range for the given layer.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>ReLU Activations:</b> Effective for layers with ReLU or similar activations, helping to keep activations and gradients in a stable range.</li>
                                                <li><b>Deep Networks:</b> Useful in deep networks to maintain uniformity and stability of weights across layers.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Activation Function Compatibility:</b> While effective for ReLU activations, this initializer might not be suitable for activation functions with different properties.</li>
                                                <li><b>Scale Sensitivity:</b> The performance can be sensitive to the scale of the initializer; other initializers might be preferable based on the specific network architecture.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>
                        </div>
                        </p>
                    </div>
                </a>

                <a href="https://engineer-ece.github.io/Keras-learn/Keras3/02.%20Layers%20API/03. Layer weight initializers/15. IdentityInitializer class/readme/chatgpt" class="card">
                    <div class="card-image">
                        <img src="https://engineer-ece.github.io/Keras-learn/Keras3/02. Layers API/03. Layer weight initializers/15. IdentityInitializer class/identity_distribution.png" alt="Example Image">
                    </div>
                    <div class="card-title">
                        <h2>IdentityInitializer</h2>
                        <button class="show-more-btn">show</button>
                    </div>
                    <div class="card-content hidden-content">
                        <p class="equation">
                            <div class="container" style="text-align: left;">
                                <h1>Identity Initializer in Keras</h1>
                            
                                <div class="section">
                                    <h2>Formula</h2>
                                    <p>The <code>Identity</code> initializer generates weights according to the identity matrix. The formula for the weights \( W \) initialized using <code>Identity</code> is:</p>
                                    <div class="equation">
                                        <p>For a weight matrix \( W \) of size \( n \times n \):</p>
                                        \[
                            W_{ij} = \begin{cases}
                            1 & \text{if } i = j \\
                            0 & \text{if } i \ne j
                            \end{cases}
                                        \]
                                        <ul>
                                            <li><b>\(W_{ij}\)</b> represents the element at the \(i\)-th row and \(j\)-th column of the weight matrix.</li>
                                            <li><b>\(n \times n\)</b> denotes a square matrix where the number of rows and columns is equal.</li>
                                        </ul>
                                    </div>
                                </div>
                            
                                <div class="section">
                                    <h2>Practical Considerations</h2>
                                    <p>Here are some practical considerations for using the <code>Identity</code> initializer:</p>
                                    <ul>
                                        <li><b>Initialization in Neural Networks</b>:
                                            <ul>
                                                <li><b>When to Use:</b> The Identity initializer is useful when you want to initialize weights such that the identity mapping is preserved. This can be particularly important in certain deep learning architectures like residual networks.</li>
                                                <li><b>Impact on Training:</b> Using the identity initializer can help with convergence by preserving the identity of the input data, particularly in very deep networks.</li>
                                            </ul>
                                        </li>
                                        <li><b>Layer Compatibility</b>:
                                            <ul>
                                                <li><b>Square Weight Matrices:</b> This initializer is only applicable to layers where the weight matrix is square (i.e., the number of input units equals the number of output units).</li>
                                            </ul>
                                        </li>
                                        <li><b>Choosing Parameters</b>:
                                            <ul>
                                                <li><b>Matrix Size:</b> Ensure that the weight matrix is square for the identity initializer to be applied correctly. For non-square matrices, other initializers might be required.</li>
                                            </ul>
                                        </li>
                                        <li><b>Examples of Use</b>:
                                            <ul>
                                                <li><b>Residual Networks:</b> Effective in residual networks where preserving the identity mapping between layers is crucial.</li>
                                                <li><b>Stability:</b> Helps maintain stability in networks where maintaining input-output relationships is important.</li>
                                            </ul>
                                        </li>
                                        <li><b>Potential Issues</b>:
                                            <ul>
                                                <li><b>Non-Square Layers:</b> Cannot be used with non-square weight matrices, which limits its application to specific types of layers.</li>
                                                <li><b>Layer Type:</b> Ensure that the layer is compatible with the identity initialization to avoid incorrect configurations.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>
                            </div>
                            
                        </p>
                    </div>
                </a>
                
                <!-- Add more cards as needed -->
            </div>

            
                <div>
                    <h1>Weight Initializers Overview</h1>
                    <table>
                        <thead>
                            <tr>
                                <th>Initializer</th>
                                <th>Formula</th>
                                <th>Range</th>
                                <th>Use Case</th>
                                <th>Tips</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>RandomNormal</td>
                                <td>\( W \sim \mathcal{N}(\text{mean}, \text{stddev}^2) \)</td>
                                <td>Mean \(\pm\) 3 × stddev</td>
                                <td>General-purpose initialization. Suitable for most layers.</td>
                                <td class="tips">Ensure mean and standard deviation are set appropriately to avoid issues with gradient scale.</td>
                            </tr>
                            <tr>
                                <td>RandomUniform</td>
                                <td>\( W \sim \text{Uniform}(a, b) \)</td>
                                <td>Between \(a\) and \(b\)</td>
                                <td>General-purpose initialization. Works well when the range \([a, b]\) is appropriate.</td>
                                <td class="tips">Adjust the range based on layer size and activation function.</td>
                            </tr>
                            <tr>
                                <td>TruncatedNormal</td>
                                <td>\( W \sim \text{TruncatedNormal}(\text{mean}, \text{stddev}) \)</td>
                                <td>Limited to \(\text{mean} \pm 2 \times \text{stddev}\)</td>
                                <td>Useful when normal initialization might lead to extreme values.</td>
                                <td class="tips">Helps stabilize training by truncating extreme values.</td>
                            </tr>
                            <tr>
                                <td>Zeros</td>
                                <td>\( W = 0 \)</td>
                                <td>Zero</td>
                                <td>Typically used for biases. Avoid for weights as it can lead to symmetry issues.</td>
                                <td class="tips">Useful for biases in some layers; avoid for weights.</td>
                            </tr>
                            <tr>
                                <td>Ones</td>
                                <td>\( W = 1 \)</td>
                                <td>One</td>
                                <td>Similar to zeros, generally avoided for weights.</td>
                                <td class="tips">Useful for biases; avoid for weights due to symmetry issues.</td>
                            </tr>
                            <tr>
                                <td>GlorotNormal</td>
                                <td>\( W \sim \mathcal{N}(0, \sqrt{\frac{2}{\text{fan_in} + \text{fan_out}}}) \)</td>
                                <td>Normal distribution scaled by \( \sqrt{\frac{2}{\text{fan_in} + \text{fan_out}}} \)</td>
                                <td>Suitable for layers with tanh or sigmoid activations.</td>
                                <td class="tips">Helps maintain stable gradients and activations.</td>
                            </tr>
                            <tr>
                                <td>GlorotUniform</td>
                                <td>\( W \sim \text{Uniform}\left(-\sqrt{\frac{6}{\text{fan_in} + \text{fan_out}}}, \sqrt{\frac{6}{\text{fan_in} + \text{fan_out}}}\right) \)</td>
                                <td>Uniform distribution scaled by \( \sqrt{\frac{6}{\text{fan_in} + \text{fan_out}}} \)</td>
                                <td>Works well with tanh or sigmoid activation functions.</td>
                                <td class="tips">Ensures proper scaling of weights for deep networks.</td>
                            </tr>
                            <tr>
                                <td>HeNormal</td>
                                <td>\( W \sim \mathcal{N}(0, \sqrt{\frac{2}{\text{fan_in}}}) \)</td>
                                <td>Normal distribution scaled by \( \sqrt{\frac{2}{\text{fan_in}}} \)</td>
                                <td>Ideal for ReLU and its variants.</td>
                                <td class="tips">Prevents vanishing gradients and helps in maintaining activation stability.</td>
                            </tr>
                            <tr>
                                <td>HeUniform</td>
                                <td>\( W \sim \text{Uniform}\left(-\sqrt{\frac{6}{\text{fan_in}}}, \sqrt{\frac{6}{\text{fan_in}}}\right) \)</td>
                                <td>Uniform distribution scaled by \( \sqrt{\frac{6}{\text{fan_in}}} \)</td>
                                <td>Similar to HeNormal but uses uniform distribution.</td>
                                <td class="tips">Effective for ReLU activations; adjust range based on network architecture.</td>
                            </tr>
                            <tr>
                                <td>Orthogonal</td>
                                <td>\( W \) is an orthogonal matrix where \( W^T W = I \)</td>
                                <td>Orthogonal matrix with unit norm</td>
                                <td>Useful in RNNs and deep networks to maintain orthogonality and stabilize training.</td>
                                <td class="tips">Helps in maintaining stability in deep networks and RNNs.</td>
                            </tr>
                            <tr>
                                <td>Constant</td>
                                <td>\( W = c \)</td>
                                <td>Constant value \(c\)</td>
                                <td>Typically used for biases rather than weights.</td>
                                <td class="tips">Useful for biases where a specific constant initialization is needed.</td>
                            </tr>
                            <tr>
                                <td>VarianceScaling</td>
                                <td>\( W \sim \text{Uniform}\left(-\sqrt{\frac{6}{\text{scale}}}, \sqrt{\frac{6}{\text{scale}}}\right) \)</td>
                                <td>Distribution scaled by a given factor</td>
                                <td>Flexible initializer for different layers and activation functions.</td>
                                <td class="tips">Adjust the scale parameter based on the network architecture and layer requirements.</td>
                            </tr>
                            <tr>
                                <td>LeCunNormal</td>
                                <td>\( W \sim \mathcal{N}(0, \sqrt{\frac{1}{\text{fan_in}}}) \)</td>
                                <td>Normal distribution scaled by \( \sqrt{\frac{1}{\text{fan_in}}} \)</td>
                                <td>Effective for Leaky ReLU and SELU activations.</td>
                                <td class="tips">Helps maintain proper gradient scaling and stability in deep networks.</td>
                            </tr>
                            <tr>
                                <td>LeCunUniform</td>
                                <td>\( W \sim \text{Uniform}\left(-\sqrt{\frac{3}{\text{fan_in}}}, \sqrt{\frac{3}{\text{fan_in}}}\right) \)</td>
                                <td>Uniform distribution scaled by \( \sqrt{\frac{3}{\text{fan_in}}} \)</td>
                                <td>Suitable for Leaky ReLU and SELU activations.</td>
                                <td class="tips">Ensures weights are initialized within a suitable range for stable training.</td>
                            </tr>
                            <tr>
                                <td>Identity</td>
                                <td>\( W \) is an identity matrix where \( W_{ij} = 1 \text{ if } i = j, 0 \text{ otherwise} \)</td>
                                <td>Identity matrix</td>
                                <td>Used for square matrices, often in residual networks.</td>
                                <td class="tips">Only applicable to square weight matrices; maintains identity mapping in specific architectures.</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                
                <div>
                    <h1>Weight Initializers Supported Tasks</h1>
                    <table>
                        <thead>
                            <tr>
                                <th>Initializer</th>
                                <th>Supported Tasks</th>
                                <th>Not Supported Tasks</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>RandomNormal</td>
                                <td class="supported">General-purpose initialization, Deep Networks, Convolutional Layers (e.g., initializing weights in CNN layers)</td>
                                <td class="not-supported">None, but might be less suitable for ReLU activations compared to He initialization</td>
                            </tr>
                            <tr>
                                <td>RandomUniform</td>
                                <td class="supported">General-purpose initialization, Deep Networks, Convolutional Layers (e.g., initializing weights in CNN layers)</td>
                                <td class="not-supported">None, but could be less effective in preventing vanishing/exploding gradients compared to specialized initializers</td>
                            </tr>
                            <tr>
                                <td>TruncatedNormal</td>
                                <td class="supported">General-purpose initialization, Networks prone to extreme values (e.g., large MLPs with dense layers)</td>
                                <td class="not-supported">None, but may be less effective for certain activation functions like ReLU compared to He initializers</td>
                            </tr>
                            <tr>
                                <td>Zeros</td>
                                <td class="supported">Bias initialization (e.g., initializing biases in any layer)</td>
                                <td class="not-supported">Weights in hidden layers (can lead to symmetry issues and poor training)</td>
                            </tr>
                            <tr>
                                <td>Ones</td>
                                <td class="supported">Bias initialization (e.g., initializing biases in any layer)</td>
                                <td class="not-supported">Weights in hidden layers (similar to zeros, can cause symmetry issues and slow training)</td>
                            </tr>
                            <tr>
                                <td>GlorotNormal</td>
                                <td class="supported">Layers with tanh or sigmoid activations (e.g., initializing weights in MLPs with these activations)</td>
                                <td class="not-supported">None, but less effective for ReLU activations compared to He initializers</td>
                            </tr>
                            <tr>
                                <td>GlorotUniform</td>
                                <td class="supported">Layers with tanh or sigmoid activations (e.g., initializing weights in MLPs with these activations)</td>
                                <td class="not-supported">None, but less effective for ReLU activations compared to He initializers</td>
                            </tr>
                            <tr>
                                <td>HeNormal</td>
                                <td class="supported">ReLU and its variants (e.g., initializing weights in layers with ReLU or Leaky ReLU activations)</td>
                                <td class="not-supported">None, but might be less effective for tanh or sigmoid activations compared to Glorot initializers</td>
                            </tr>
                            <tr>
                                <td>HeUniform</td>
                                <td class="supported">ReLU and its variants (e.g., initializing weights in layers with ReLU or Leaky ReLU activations)</td>
                                <td class="not-supported">None, but might be less effective for tanh or sigmoid activations compared to Glorot initializers</td>
                            </tr>
                            <tr>
                                <td>Orthogonal</td>
                                <td class="supported">RNNs, Deep Networks requiring orthogonality (e.g., initializing weights in RNN layers to maintain stability)</td>
                                <td class="not-supported">Non-square weight matrices (e.g., layers where the number of input and output units are not equal)</td>
                            </tr>
                            <tr>
                                <td>Constant</td>
                                <td class="supported">Bias initialization (e.g., initializing biases in any layer with a specific constant value)</td>
                                <td class="not-supported">Weights in hidden layers (e.g., initializing weights in hidden layers can lead to symmetry issues and poor training)</td>
                            </tr>
                            <tr>
                                <td>VarianceScaling</td>
                                <td class="supported">General-purpose initialization, Flexible scaling for different layers (e.g., initializing weights in a variety of architectures)</td>
                                <td class="not-supported">None, but may be less specific for certain activations compared to dedicated initializers</td>
                            </tr>
                            <tr>
                                <td>LeCunNormal</td>
                                <td class="supported">Leaky ReLU, SELU activations (e.g., initializing weights in layers with Leaky ReLU or SELU activations)</td>
                                <td class="not-supported">None, but might be less effective for ReLU or sigmoid activations compared to He or Glorot initializers</td>
                            </tr>
                            <tr>
                                <td>LeCunUniform</td>
                                <td class="supported">Leaky ReLU, SELU activations (e.g., initializing weights in layers with Leaky ReLU or SELU activations)</td>
                                <td class="not-supported">None, but might be less effective for ReLU or sigmoid activations compared to He or Glorot initializers</td>
                            </tr>
                            <tr>
                                <td>Identity</td>
                                <td class="supported">Square weight matrices, Residual Networks (e.g., initializing weights in residual connections where the matrix is square)</td>
                                <td class="not-supported">Non-square weight matrices (e.g., layers where the number of input and output units are not equal)</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            
                

                <div>
                    <h1>Weight Initializers Supported Layers</h1>
                    <table>
                        <thead>
                            <tr>
                                <th>Initializer</th>
                                <th>Supported Layers</th>
                                <th>Not Supported Layers</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>RandomNormal</td>
                                <td class="supported">Dense layers, Convolutional layers, LSTM layers (when used with general-purpose initialization)</td>
                                <td class="not-supported">None, but may be less optimal for layers with ReLU activations compared to He initializers</td>
                            </tr>
                            <tr>
                                <td>RandomUniform</td>
                                <td class="supported">Dense layers, Convolutional layers, LSTM layers (when used with general-purpose initialization)</td>
                                <td class="not-supported">None, but could be less effective in preventing vanishing/exploding gradients compared to specialized initializers</td>
                            </tr>
                            <tr>
                                <td>TruncatedNormal</td>
                                <td class="supported">Dense layers, Convolutional layers, RNN layers (helps with networks prone to extreme values)</td>
                                <td class="not-supported">None, but may not be as effective for ReLU activations compared to He initializers</td>
                            </tr>
                            <tr>
                                <td>Zeros</td>
                                <td class="supported">Bias layers (initialization of biases in any layer)</td>
                                <td class="not-supported">Weights in hidden layers (can lead to symmetry issues and poor learning)</td>
                            </tr>
                            <tr>
                                <td>Ones</td>
                                <td class="supported">Bias layers (initialization of biases in any layer)</td>
                                <td class="not-supported">Weights in hidden layers (can lead to symmetry issues and poor learning)</td>
                            </tr>
                            <tr>
                                <td>GlorotNormal</td>
                                <td class="supported">Dense layers, Convolutional layers with tanh or sigmoid activations</td>
                                <td class="not-supported">None, but might be less effective for ReLU activations compared to He initializers</td>
                            </tr>
                            <tr>
                                <td>GlorotUniform</td>
                                <td class="supported">Dense layers, Convolutional layers with tanh or sigmoid activations</td>
                                <td class="not-supported">None, but might be less effective for ReLU activations compared to He initializers</td>
                            </tr>
                            <tr>
                                <td>HeNormal</td>
                                <td class="supported">Dense layers, Convolutional layers, especially with ReLU or Leaky ReLU activations</td>
                                <td class="not-supported">None, but less effective for tanh or sigmoid activations compared to Glorot initializers</td>
                            </tr>
                            <tr>
                                <td>HeUniform</td>
                                <td class="supported">Dense layers, Convolutional layers, especially with ReLU or Leaky ReLU activations</td>
                                <td class="not-supported">None, but less effective for tanh or sigmoid activations compared to Glorot initializers</td>
                            </tr>
                            <tr>
                                <td>Orthogonal</td>
                                <td class="supported">RNN layers, Layers requiring orthogonality (e.g., certain types of recurrent networks)</td>
                                <td class="not-supported">Non-square weight matrices (e.g., layers where input and output units differ)</td>
                            </tr>
                            <tr>
                                <td>Constant</td>
                                <td class="supported">Bias layers (initialization of biases in any layer with a constant value)</td>
                                <td class="not-supported">Weights in hidden layers (can lead to symmetry issues and poor learning)</td>
                            </tr>
                            <tr>
                                <td>VarianceScaling</td>
                                <td class="supported">General-purpose initialization, Suitable for various types of layers with flexible scaling</td>
                                <td class="not-supported">None, but less specific for certain activation functions compared to dedicated initializers</td>
                            </tr>
                            <tr>
                                <td>LeCunNormal</td>
                                <td class="supported">Dense layers, Convolutional layers with Leaky ReLU or SELU activations</td>
                                <td class="not-supported">None, but might be less effective for ReLU or sigmoid activations compared to He or Glorot initializers</td>
                            </tr>
                            <tr>
                                <td>LeCunUniform</td>
                                <td class="supported">Dense layers, Convolutional layers with Leaky ReLU or SELU activations</td>
                                <td class="not-supported">None, but might be less effective for ReLU or sigmoid activations compared to He or Glorot initializers</td>
                            </tr>
                            <tr>
                                <td>IdentityInitializer</td>
                                <td class="supported">Layers with square weight matrices, Residual networks (where identity mapping is beneficial)</td>
                                <td class="not-supported">Non-square weight matrices (e.g., layers where input and output units differ)</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <div class="container">
                    <h1>Weight Initializers: Supported and Not Supported Tasks</h1>
                    <ul>
                        <li>
                            <strong>RandomNormal:</strong> 
                            <ul>
                                <li><strong>Supported:</strong> General-purpose initialization for dense, convolutional, and LSTM layers.</li>
                                <li><strong>Not Supported:</strong> Tasks requiring specific initialization tailored to certain activation functions, such as ReLU where He initializers might be better.</li>
                            </ul>
                        </li>
                        <li>
                            <strong>RandomUniform:</strong> 
                            <ul>
                                <li><strong>Supported:</strong> General-purpose use in dense and convolutional layers.</li>
                                <li><strong>Not Supported:</strong> Scenarios where preventing vanishing or exploding gradients is critical, and specialized initializers might be needed.</li>
                            </ul>
                        </li>
                        <li>
                            <strong>TruncatedNormal:</strong> 
                            <ul>
                                <li><strong>Supported:</strong> Networks prone to extreme values, dense, convolutional, and RNN layers.</li>
                                <li><strong>Not Supported:</strong> ReLU activations where other initializers may perform better.</li>
                            </ul>
                        </li>
                        <li>
                            <strong>Zeros:</strong> 
                            <ul>
                                <li><strong>Supported:</strong> Bias initialization.</li>
                                <li><strong>Not Supported:</strong> Weight initialization in hidden layers due to symmetry breaking issues.</li>
                            </ul>
                        </li>
                        <li>
                            <strong>Ones:</strong> 
                            <ul>
                                <li><strong>Supported:</strong> Bias initialization.</li>
                                <li><strong>Not Supported:</strong> Weight initialization in hidden layers due to symmetry issues.</li>
                            </ul>
                        </li>
                        <li>
                            <strong>GlorotNormal & GlorotUniform:</strong> 
                            <ul>
                                <li><strong>Supported:</strong> Layers with tanh or sigmoid activations.</li>
                                <li><strong>Not Supported:</strong> Layers with ReLU activations where He initializers are preferred.</li>
                            </ul>
                        </li>
                        <li>
                            <strong>HeNormal & HeUniform:</strong> 
                            <ul>
                                <li><strong>Supported:</strong> Layers with ReLU or its variants.</li>
                                <li><strong>Not Supported:</strong> Layers with tanh or sigmoid activations.</li>
                            </ul>
                        </li>
                        <li>
                            <strong>Orthogonal:</strong> 
                            <ul>
                                <li><strong>Supported:</strong> RNNs and layers requiring orthogonality.</li>
                                <li><strong>Not Supported:</strong> Non-square weight matrices.</li>
                            </ul>
                        </li>
                        <li>
                            <strong>Constant:</strong> 
                            <ul>
                                <li><strong>Supported:</strong> Bias initialization.</li>
                                <li><strong>Not Supported:</strong> Weight initialization due to symmetry issues.</li>
                            </ul>
                        </li>
                        <li>
                            <strong>VarianceScaling:</strong> 
                            <ul>
                                <li><strong>Supported:</strong> Versatile across various layer types.</li>
                                <li><strong>Not Supported:</strong> Specific activations requiring more tailored initialization methods.</li>
                            </ul>
                        </li>
                        <li>
                            <strong>LeCunNormal & LeCunUniform:</strong> 
                            <ul>
                                <li><strong>Supported:</strong> Layers with Leaky ReLU or SELU activations.</li>
                                <li><strong>Not Supported:</strong> Layers with ReLU or sigmoid activations.</li>
                            </ul>
                        </li>
                        <li>
                            <strong>IdentityInitializer:</strong> 
                            <ul>
                                <li><strong>Supported:</strong> Square weight matrices and residual networks.</li>
                                <li><strong>Not Supported:</strong> Non-square matrices.</li>
                            </ul>
                        </li>
                    </ul>
                </div>
                <div class="container">
                    <h1>Weight Initializers: Supported and Not Supported Layers</h1>
                    <ul>
                        <li>
                            <strong>RandomNormal:</strong> Suitable for general-purpose initialization, applicable to dense, convolutional, and LSTM layers. It might be less optimal for ReLU activations compared to He initializers.
                        </li>
                        <li>
                            <strong>RandomUniform:</strong> Similar to RandomNormal, it’s good for general-purpose use but might be less effective in preventing vanishing or exploding gradients compared to specialized initializers.
                        </li>
                        <li>
                            <strong>TruncatedNormal:</strong> Helps with networks prone to extreme values, effective in dense, convolutional, and RNN layers. Less effective for ReLU activations.
                        </li>
                        <li>
                            <strong>Zeros:</strong> Best for initializing biases, but not recommended for weights in hidden layers due to potential symmetry issues.
                        </li>
                        <li>
                            <strong>Ones:</strong> Like Zeros, useful for bias initialization but problematic for weights in hidden layers.
                        </li>
                        <li>
                            <strong>GlorotNormal & GlorotUniform:</strong> Effective for layers with tanh or sigmoid activations. Less suited for ReLU activations compared to He initializers.
                        </li>
                        <li>
                            <strong>HeNormal & HeUniform:</strong> Optimal for layers with ReLU or its variants. Less effective for tanh or sigmoid activations.
                        </li>
                        <li>
                            <strong>Orthogonal:</strong> Useful for RNNs and layers requiring orthogonality but not suitable for non-square weight matrices.
                        </li>
                        <li>
                            <strong>Constant:</strong> Good for bias initialization but can cause issues when used for weights due to symmetry.
                        </li>
                        <li>
                            <strong>VarianceScaling:</strong> Versatile for various types of layers, though less specific for certain activations compared to dedicated initializers.
                        </li>
                        <li>
                            <strong>LeCunNormal & LeCunUniform:</strong> Ideal for layers with Leaky ReLU or SELU activations. Less effective for ReLU or sigmoid activations.
                        </li>
                        <li>
                            <strong>IdentityInitializer:</strong> Useful for square weight matrices and residual networks but not for non-square matrices.
                        </li>
                    </ul>
                </div>
            
        </main>
       
    </div>
    
</body>
</html>
